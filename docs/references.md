# References

This project is part of my self-learning path in Audio Machine Learning.  
Here are the main sources and resources that helped me.

---

### Video Tutorials
- **The Sound of AI - Valerio Velardo (YouTube)**  
  I followed this complete tutorial series to learn about artificial neurons, MLPs, and CNNs for sound.

---

### Audio (ðŸŽ§) Datasets
- **UrbanSound8K Dataset**  
  https://urbansounddataset.weebly.com/urbansound8k.html  
  This dataset contains 8732 short sound clips of urban noises (car horn, dog bark, drilling, etc.).  
  I used a few folds as examples in my notebooks.

---

### Setup (ðŸ§°) Tools and Libraries
- Python 3.8  
- NumPy, Pandas  
- TensorFlow / Keras  
- Librosa (for audio feature extraction)  
- Matplotlib and Seaborn (for visualization)

look at the requirements.txt file. All you have to do is 

```
python3 -m venv venv # making a virtual environment named as venv
source venv/bin/activate # activate the virtual environment
pip install -r requirements.txt
```
---

### My Setup
All notebooks were tested on Ubuntu with Python 3.8 and TensorFlow 2.x.  
Plots and MFCC/Mel visualizations were generated using Librosa and Matplotlib.

